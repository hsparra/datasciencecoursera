words_needed <- min(which(df$cum_count >= val * percent))
src <- df$src[1]
data.frame(Source = "twitter",
#Coverage = paste(percent * 100, "%", sep=""),
Words_Needed= words_needed,
Percent = words_needed / length(df$word))
}
coverage_df <- coverageNums(t_df, .5)
coverage_df <- coverageNums(t_df, .5) %>%
rbind(coverageNums(t_df, .9))
coverage_df
coverageNums <- function( df, percent) {
val <- tail(df$cum_count, 1)
words_needed <- min(which(df$cum_count >= val * percent))
src <- df$src[1]
data.frame(Source = "twitter",
Coverage = paste(percent * 100, "%", sep="")
Words_Needed= words_needed,
Percent = words_needed / length(df$word))
}
coverageNums <- function( df, percent) {
val <- tail(df$cum_count, 1)
words_needed <- min(which(df$cum_count >= val * percent))
src <- df$src[1]
data.frame(Source = "twitter",
Coverage = paste(percent * 100, "%", sep=""),
Words_Needed= words_needed,
Percent = words_needed / length(df$word))
}
coverage_df <- coverageNums(t_df, .5) %>%
rbind(coverageNums(t_df, .9))
coverage_df
coverageNums <- function( df, percent) {
val <- tail(df$cum_count, 1)
words_needed <- min(which(df$cum_count >= val * percent))
src <- df$src[1]
data.frame(Source = "twitter",
Coverage = paste(percent * 100, "%", sep=""),
Words_Needed= words_needed,
Percent = paste(words_needed / length(df$word), "%", sep="")
}
coverageNums <- function( df, percent) {
val <- tail(df$cum_count, 1)
words_needed <- min(which(df$cum_count >= val * percent))
src <- df$src[1]
data.frame(Source = "twitter",
Coverage = paste(percent * 100, "%", sep=""),
Words_Needed= words_needed,
Percent = paste(words_needed / length(df$word), "%", sep=""))
}
coverage_df <- coverageNums(t_df, .5) %>% rbind(coverageNums(t_df, .9)) %>%
rbind(coverageNums(b_df, .5)) %>% rbind(coverageNums(b_df, .9)) %>%
rbind(coverageNums(n_df, .5)) %>% rbind(coverageNums(n_df, .9))
coverage_df
coverageNums <- function( df, percent) {
val <- tail(df$cum_count, 1)
words_needed <- min(which(df$cum_count >= val * percent))
src <- df$src[1]
data.frame(Source = "twitter",
Coverage = paste(percent * 100, "%", sep=""),
Words_Needed= words_needed,
Percent = paste(words_needed / length(df$word) * 100, "%", sep=""))
}
coverage_df <- coverageNums(t_df, .5) %>% rbind(coverageNums(t_df, .9)) %>%
rbind(coverageNums(b_df, .5)) %>% rbind(coverageNums(b_df, .9)) %>%
rbind(coverageNums(n_df, .5)) %>% rbind(coverageNums(n_df, .9))
coverage_df
coverageNums <- function( df, percent) {
val <- tail(df$cum_count, 1)
words_needed <- min(which(df$cum_count >= val * percent))
src <- df$src[1]
data.frame(Source = "twitter",
Coverage = paste(percent * 100, "%", sep=""),
Words_Needed= words_needed,
Percent = format(paste(words_needed / length(df$word) * 100, digits=0), "%", sep=""))
}
coverage_df <- coverageNums(t_df, .5) %>% rbind(coverageNums(t_df, .9)) %>%
rbind(coverageNums(b_df, .5)) %>% rbind(coverageNums(b_df, .9)) %>%
rbind(coverageNums(n_df, .5)) %>% rbind(coverageNums(n_df, .9))
coverage_df
coverageNums <- function( df, percent) {
val <- tail(df$cum_count, 1)
words_needed <- min(which(df$cum_count >= val * percent))
src <- df$src[1]
data.frame(Source = "twitter",
Coverage = paste(percent * 100, "%", sep=""),
Words_Needed= words_needed,
Percent = paste(format(words_needed / length(df$word) * 100, digits=0), "%", sep=""))
}
coverage_df <- coverageNums(t_df, .5) %>% rbind(coverageNums(t_df, .9)) %>%
rbind(coverageNums(b_df, .5)) %>% rbind(coverageNums(b_df, .9)) %>%
rbind(coverageNums(n_df, .5)) %>% rbind(coverageNums(n_df, .9))
coverage_df
coverageNums <- function( df, percent) {
val <- tail(df$cum_count, 1)
words_needed <- min(which(df$cum_count >= val * percent))
src <- df$src[1]
data.frame(Source = "twitter",
Coverage = paste(percent * 100, "%", sep=""),
Words_Needed= words_needed,
Percent_of_Unique_Words = paste(format(words_needed / length(df$word) * 100, digits=0), "%", sep=""))
}
coverage_df <- coverageNums(t_df, .5) %>% rbind(coverageNums(t_df, .9)) %>%
rbind(coverageNums(b_df, .5)) %>% rbind(coverageNums(b_df, .9)) %>%
rbind(coverageNums(n_df, .5)) %>% rbind(coverageNums(n_df, .9))
coverage_df
coverageNums <- function( df, percent) {
val <- tail(df$cum_count, 1)
words_needed <- min(which(df$cum_count >= val * percent))
src <- df$src[1]
data.frame(Source = src,
Coverage = paste(percent * 100, "%", sep=""),
Words_Needed= words_needed,
Percent_of_Unique_Words = paste(format(words_needed / length(df$word) * 100, digits=0), "%", sep=""))
}
coverage_df <- coverageNums(t_df, .5) %>% rbind(coverageNums(t_df, .9)) %>%
rbind(coverageNums(b_df, .5)) %>% rbind(coverageNums(b_df, .9)) %>%
rbind(coverageNums(n_df, .5)) %>% rbind(coverageNums(n_df, .9))
coverage_df
coverageNums <- function( df, percent) {
val <- tail(df$cum_count, 1)
words_needed <- min(which(df$cum_count >= val * percent))
src <- df$src[1]
data.frame(Source = src,
Coverage = paste(percent * 100, "%", sep=""),
Words_Needed= words_needed,
Percent_of_Unique_Words = paste(format(words_needed / length(df$word) * 100, digits=0), "%", sep=""))
}
coverage_df <- coverageNums(t_df, .5) %>% rbind(coverageNums(b_df, .5)) %>% rbind(coverageNums(n_df, .5)) %>%
rbind(coverageNums(t_df, .9)) %>% rbind(coverageNums(b_df, .9)) %>%rbind(coverageNums(n_df, .9))
coverage_df
x <- "This is to see how a particular strsplit will work"
strsplit(x, " .* ")
strsplit(x, "[] .* ]")
strsplit(x, "[ .* ]")
strsplit(x, "[ a-z* ]")
strsplit(x, " a-z* ")
strsplit(x, " a-z+ ")
strsplit(x, "[ a-z+ ]")
strsplit(x, "[a-z+ a-z+]")
strsplit(x, " [a-z]+ [a-z]+")
strsplit(x, " [a-z]+ [a-z]+ ")
strsplit(x, "\W")
strsplit(x, "\\W")
strsplit(x, "\\W{2}")
strsplit(x, "\\W{2,2}")
strsplit(x, "\\W")
strsplit(x, "\\S")
strsplit(x, "\S")
strsplit(x, "\\s+")
strsplit(x, "\\s+ \\s+")
strsplit(x, "\\s+ ")
strsplit(x, "\\s+")
y <- strsplit(x, "\\s+")
class(y)
y
y <- unlist(strsplit(x, "\\s+"))
class(y)
y
z <- collapse(y[seq_along(1, length(y) - 1), y[seq_along(2, length(y))]])
seq_along(1)
seq_along(2)
z <- collapse(y[seq(1, length(y) - 1), y[seq(2, length(y))]])
z <- collapse(y[seq(from=1, to=length(y) - 1, by=1), y[seq(from=2, to=length(y), by=1)]])
z <- collapse(y[seq(from=1, to=length(y) - 1, by=1)], y[seq(from=2, to=length(y), by=1)])
collapse(y[1], y[2])
z <- paste(y[seq(from=1, to=length(y) - 1, by=1)], y[seq(from=2, to=length(y), by=1)], sep = " ")
z
createNGram <- function(inSentence, ngamNum = 2) {
x <- unlist(strsplit(x, "\\s+"))
paste(x[seq(from=1, to=length(y) - 1, by=1)], x[seq(from=2, to=length(y), by=1)], sep = " ")
}
createNGram("This is a sentence to tet my new nGram creator")
createNGram <- function(inSentence, ngamNum = 2) {
x <- unlist(strsplit(x, "\\s+"))
paste(x[seq(from=1, to=length(x) - 1, by=1)], x[seq(from=2, to=length(x), by=1)], sep = " ")
}
createNGram("This is a sentence to tet my new nGram creator")
createNGram
createNGram <- function(inSentence, ngamNum = 2) {
x <- unlist(strsplit(inSentence, "\\s+"))
paste(x[seq(from=1, to=length(x) - 1, by=1)], x[seq(from=2, to=length(x), by=1)], sep = " ")
}
createNGram
createNGram("This is a sentence to tet my new nGram creator")
createNGram <- function(inSentence, ngamNum = 2) {
x <- unlist(strsplit(inSentence, "\\s+"))
paste(x[seq(from=1, to=length(x) - 1, by=1)], x[seq(from=ngramNum, to=length(x), by=1)], sep = " ")
}
createNGram("This is a sentence to tet my new nGram creator", 2)
createNGram("This is a sentence to tet my new nGram creator")
createNGram <- function(inSentence, ngramNum = 2) {
x <- unlist(strsplit(inSentence, "\\s+"))
paste(x[seq(from=1, to=length(x) - 1, by=1)], x[seq(from=ngramNum, to=length(x), by=1)], sep = " ")
}
createNGram("This is a sentence to tet my new nGram creator", 2)
createNGram("This is a sentence to tet my new nGram creator", 3)
one_gram <- textcnt(t_samp, method="string", n=2L)
str(one_gram)
head(one_gram)
head(x)
x
x
x[2] <- "A second sentence for my test"
x
paste(unlist(strsplit(x, split=" ")), sep=" ")
paste(unlist(strsplit(x, split=" ")), collapse =" ")
lapply(x, createNGram)
x <- c("A first line to parse", "A second line to parse", "Yet another something to work with")
lapply(x, createNGram)
sapply(x, createNGram)
lapply(x, createNGram)
y <- lapply(x, createNGram)
y[1][1]
y[[1]]
y[[1]][1]
as.data.frame(y)
as.matrix(y)
y
x <- c("A first line to parse", "A second line to parse", "Yet another something to work #2 with!!")
y <- sapply(x, createNGram)   # will create vector of NGrams
y
y <- sapply(x, cleanText) %>% createNGram
y <- sapply(x, cleanText)
y
class(y)
as.vector(y)
y <- sapply(x, cleanText) %>% as.vector %>% createNGram
createNGram <- function(inSentence, ngramNum = 2) {
#x <- unlist(strsplit(inSentence, "\\s+"))
x <- unlist(strsplit(inSentence, " "))
paste(x[seq(from=1, to=length(x) - 1, by=1)], x[seq(from=2, to=length(x), by=1)], sep = " ")
}
y <- sapply(x, cleanText) %>% createNGram
y <- sapply(x, cleanText) %>% as.vector %>% createNGram
y <- sapply(x, cleanText) %>% as.vector
class(y)
unlist(y)
y
y <- sapply(x, cleanText)
y
unlist(y)
z <- unlist(y)
z
length(z)
dim(z)
z[1:2]
z[1:5]
y <- apply(x, cleanText)
y
z
names(z) <- NULL
z
z <- sapply(x, cleanText) %>% unlist %>% createNGram
z
x
createNGram(x[1])
unlist(y[[1]])
y[1]
y
z <- sapply(x, cleanText) %>% unlist %>% createNGram
z
z <- sapply(x, cleanText) %>% %>% sapply(createNGram)
z <- sapply(x, cleanText) %>% sapply(createNGram)
z
z <- sapply(x, cleanText) %>% sapply(createNGram) %>% unlist
z
names(z) <- NULL
z
x
z <- sapply(x, cleanText) %>% sapply(createNGram) %>% unlist %>% names <- NULL
z <- sapply(x, cleanText) %>% sapply(createNGram) %>% unlist %>% function(x) names(x) <- NULL
z
z <- sapply(x, cleanText) %>% sapply(createNGram) %>% unlist
z
names(y) <- NULL
y
createNGram <- function(inSentence, ngramNum = 2) {
#x <- unlist(strsplit(inSentence, "\\s+"))
x <- unlist(strsplit(inSentence, " "))
y <- paste(x[seq(from=1, to=length(x) - 1, by=1)], x[seq(from=2, to=length(x), by=1)], sep = " ")
names(y) <- NULL
}
z <- sapply(x, cleanText) %>% sapply(createNGram) %>% unlist
z
createNGram <- function(inSentence, ngramNum = 2) {
#x <- unlist(strsplit(inSentence, "\\s+"))
x <- unlist(strsplit(inSentence, " "))
y <- paste(x[seq(from=1, to=length(x) - 1, by=1)], x[seq(from=2, to=length(x), by=1)], sep = " ")
names(y) <- NULL
y
}
z <- sapply(x, cleanText) %>% sapply(createNGram) %>% unlist
z
head(bigrams)
bigrams <- NGramTokenizer(t_samp, control = Weka_control(min=2, max=2, dilimiters = " '"))
library(RWeka)
bigrams <- NGramTokenizer(t_samp, control = Weka_control(min=2, max=2, dilimiters = " '"))
head(bigrams)
x
gsub("[:punct:]", "NADA", x)
x
gsub(":punct:", "NADA", x)
gsub("[[:punct:]]", "NADA", x)
cleanText <- function(data) {
data <- unlist(strsplit(data, split=" "))   %>%
tolower                                 %>%
remove_stopwords(stopwords())           %>%
function (x) gsub("[[:punct:]]", " ", x) %>%
function (x) gsub("[^a-z]", "", x)      %>%
function (x) gsub("[ ]", "", x)         %>%
function (x) x[ x != ""]                %>%
wordStem
}
z <- sapply(x, cleanText) %>% sapply(createNGram) %>% unlist
z
str(bigrams)
class(bigrams)
t_clean <- cleanText(t_samp)
head(t_clean)
cleanText <- function(data) {
data <- unlist(strsplit(data, split=" "))   %>%
tolower                                 %>%
remove_stopwords(stopwords())           %>%
function (x) gsub("[[:punct:]]", " ", x) %>%
function (x) gsub("[^a-z]", "", x)      %>%
#function (x) gsub("[ ]", "", x)         %>%
function (x) x[ x != ""]                %>%
wordStem
}
t_clean <- cleanText(t_samp)
head(t_clean)
cleanText <- function(data) {
data <- unlist(strsplit(data, split=" "))   %>%
tolower                                 %>%
remove_stopwords(stopwords())           %>%
function (x) gsub("[[:punct:]]", " ", x) %>%
function (x) gsub("[^a-z]", " ", x)      %>%
#function (x) gsub("[ ]", "", x)         %>%
function (x) x[ x != ""]                %>%
wordStem
}
t_clean <- cleanText(t_samp)
head(t_clean)
bigrams <- NGramTokenizer(t_clean, control = Weka_control(min=2, max=2, dilimiters = " '"))
head(bigrams)
t_df[t_df$word == "http",]
t_df[t_df$word == "www",]
t_words <- cleanText(t_samp)
t_df <- createTableOfCounts(t_words, "twitter")
t_df[t_df$word == "www",]
t_df[t_df$word == "http",]
head(t_df)
cleanText <- function(data) {
data <- unlist(strsplit(data, split=" "))   %>%
tolower                                 %>%
remove_stopwords(stopwords())           %>%
function (x) gsub("[[:punct:]]", " ", x) %>%
function (x) gsub("[^a-z]", " ", x)      %>%
function (x) gsub("[ ]", "", x)         %>%
function (x) x[ x != ""]                %>%
wordStem
}
t_words <- cleanText(t_samp)
t_df <- createTableOfCounts(t_words, "twitter")
head(t_df)
bigrams <- NGramTokenizer(t_clean, control = Weka_control(min=2, max=2, dilimiters = " '"))
head(bigrams)
z <- sapply(t_samp, cleanText) %>% sapply(createNGram) %>% unlist  # create vector of NGrams
sapply(x, cleanText)
class(sapply(x, cleanText))
z <- sapply(t_samp, cleanText)
head(z)
t_samp(1)
t_samp[1]
cleanText(t_samp[1])
za <- cleanText(t_samp[1])
za
cleanText <- function(data) {
data <- unlist(strsplit(data, split=" "))   %>%
tolower                                 %>%
remove_stopwords(stopwords())           %>%
function (x) gsub("[[:punct:]]", " ", x) %>%
function (x) gsub("[^a-z]", " ", x)      %>%
function (x) gsub("[ ]{2}", " ", x)         %>%
function (x) x[ x != ""]                %>%
wordStem
}
za <- cleanText(t_samp[1])
za
head(z)
za <- createNGram(z)
createNGram <- function(inSentence, ngramNum = 2) {
x <- unlist(strsplit(inSentence, "\\S "))
y <- paste(x[seq(from=1, to=length(x) - 1, by=1)], x[seq(from=2, to=length(x), by=1)], sep = " ")
}
za <- createNGram(z)
class(z)
za <- sapply(z, createNGram) %>% unlist
za <- sapply(x, createNGram) %>% unlist
za <- unlist(za) %>% createNGram
head(za)
head(z)
head(unlist(z))
names(z) <- NULL
head(unlist(z))
head(za)
za <- unlist(z) %>% createNGram
head(zaq)
head(za)
head(z)
z <- sapply(t_samp, cleanText)
head(z)
cleanText <- function(data) {
data <- unlist(strsplit(data, split=" "))   %>%
tolower                                 %>%
remove_stopwords(stopwords())           %>%
function (x) gsub("[[:punct:]]", " ", x) %>%
function (x) gsub("[^a-z]", " ", x)      %>%
function (x) gsub("[ ]{2}", " ", x)         %>%
function (x) x[ x != " "]                %>%
wordStem
}
cleanText <- function(data, split=FALSE) {
data <- unlist(strsplit(data, split=" "))   %>%
tolower                                 %>%
remove_stopwords(stopwords())           %>%
function (x) gsub("[[:punct:]]", " ", x) %>%
function (x) gsub("[^a-z]", " ", x)      %>%
function (x) gsub("[ ]{2}", " ", x)         %>%
function (x) x[ x != " "]                %>%
wordStem
if (split) {
data <- paste(data, collapse = " ")
}
}
z <- sapply(t_samp, cleanText, TRUE)
head(z)
names(z) <- NULL
head(z)
t_words <- cleanText(t_samp)
head(t_words)
cleanText <- function(data, split=FALSE) {
data <- unlist(strsplit(data, split=" "))   %>%
tolower                                 %>%
remove_stopwords(stopwords())           %>%
function (x) gsub("[[:punct:]]", " ", x) %>%
function (x) gsub("[^a-z]", " ", x)      %>%
function (x) gsub("[ ]{2}", " ", x)         %>%
function (x) x[ x != " "]                %>%
wordStem
if (split) {
data <- paste(data, collapse = " ")
}
data
}
t_words <- cleanText(t_samp)
head(t_words)
head(t_words, 20)
t_words2 <- strsplit(t_words, split=" ")
head(t_words2, 20)
t_words2 <- unlist(strsplit(t_words, split=" "))
head(t_words2, 20)
head(z)
cleanText <- function(data, split=FALSE) {
data <- unlist(strsplit(data, split=" "))   %>%
tolower                                 %>%
remove_stopwords(stopwords())           %>%
function (x) gsub("[[:punct:]]", " ", x) %>%
function (x) gsub("[^a-z]", " ", x)      %>%
function (x) gsub("[ ]{2}", " ", x)         %>%
function (x) x[ x != " "]                %>%
wordStem
if (split) {
data <- paste(data, collapse = " ")
} else {
data <- unlist(strsplit(data, split = " "))
}
data
}
t_words <- cleanText(t_samp)
head(t_words, 20)
z <- sapply(t_samp, cleanText, TRUE)
head(z)
suppressMessages(library(RWeka))
bigrams <- textcnt(z, method="string", n=2L)
head(bigrams)
bigram_rw <- NGramTokenizer(z, control=Weka_control(min=2, max=2, dilimeters = " "))
head(bigram_rw)
t_bi <- createTableOfCounts(bigram_rw, "twitter")
head(t_bi)
b_bi <- sapply(b_samp, cleanText, TRUE) %>%>
NGramTokenizer(z, control=Weka_control(min=2, max=2, dilimeters = " ")) %>%
createTableOfCounts("blogs")
b_bi <- sapply(b_samp, cleanText, TRUE) %>%
NGramTokenizer(z, control=Weka_control(min=2, max=2, dilimeters = " ")) %>%
createTableOfCounts("blogs")
b_bi <- sapply(b_samp, cleanText, TRUE) %>%
NGramTokenizer(control=Weka_control(min=2, max=2, dilimeters = " ")) %>%
createTableOfCounts("blogs")
head(b_bi)
n_bi <- sapply(n_samp, cleanText, TRUE) %>%
NGramTokenizer(control=Weka_control(min=2, max=2, dilimeters = " ")) %>%
createTableOfCounts("news")
dim(n_bi)
bi_df <- rbind(t_bi[1:500], b_bi[1:500], n_bi[1:500])
ggplot(bi_dfdf, aes(x=index, y=count)) + geom_line() + facet_grid(. ~ src)
ggplot(bi_df, aes(x=index, y=count)) + geom_line() + facet_grid(. ~ src)
coverage_bi <- coverageNums(t_bi, .5) %>% rbind(coverageNums(b_bi, .5)) %>% rbind(coverageNums(n_bi, .5)) %>%
rbind(coverageNums(t_bi, .9)) %>% rbind(coverageNums(b_bi, .9)) %>%rbind(coverageNums(n_bi, .9))
coverage_bi
rm(list=ls())
