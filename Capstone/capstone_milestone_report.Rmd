---
title: "Capstone Milestone Report"
output: html_document
---
This report explores the initial exploratory analysis of the English text corpus for the Data Science Capstone course.

# Explore the Data
For this project we are given three files from different sources. See the *Supplemental Information* section for the R code to load the data and create the small er sample files we use for this analysis.

First we load in the data and get some line counts.
```{r}
library(tau)
library(ggpot2)
library(magrittr)
# load the data
t_samp <- readLines("twit_samp.txt")
b_samp <- readLines("blog_samp.txt")
n_samp <- readLines("news_samp.txt")

lines_per_corpus <- c(length(t_samp), length(b_samp), length(n_samp))
```

Clean up the data a little, such as getting rid of punctuation and non-letter charactrs, then get word counts.
```{r}
cleanText <- function(data) {
    data <- tolower(data)
    
    
    data <- gsub("/|@|\\|", " ", data)
    data <- gsub("[0-9]", "", data)       # remove numbers
    #data <- gsub("[a-z]'s ", " is", data)
    # remove text in brackets since probably has different context
    #data <- c(gsub("[(].+?[)]", "", data), regmatches(data, regexpr("[(].+?[)]", data)))
    # exclude punctuations
    #data <- remove_stopwords(sm_t, stopwords(kind = "english"), lines=TRUE)
    #data <- gsub("\\]", " ", gsub("\\[", " ", gsub("[…|•“”!\"#&$%\\(\\)*+./:;<=>?@^_`\\{|\\}~,'/\\-]", " ", data)))
    #data <- gsub("[…|•“”!\"#&$%\\(\\)*+./:;<=>?@^_`\\{|\\}~,'/\\-]", " ", data)
    data <- gsub("[^a-z]", " ", data)
    #data <- gsub("\\[", " ", data)
    #data <- gsub("\\]", " ", data)
    data <- iconv(data, "UTF-8", "ASCII", sub=" UNICODE ")
    data <- gsub("[UNICODE]{2,}", " UNICODE", data)
    data <- gsub("[ ]{2, }", " ", data)         # remove extra whitespaces
    data <- remove_stopwords(data, stopwords(kind="english"))
}

t_clean <- cleanText(t_samp)
b_clean <- cleanText(b_samp)
n_clean <- cleanText(n_samp)

t_words <- c(unlist(strsplit(t_clean, split=" ")))
b_words <- c(unlist(strsplit(b_clean, split=" ")))
n_words <- c(unlist(strsplit(n_clean, split=" ")))

counts <- data.frame(measure = "Number of words",twitter = length(t_words), blogs = length(b_words), news = length(n_words))
counts <- rbind(counts, data.frame(measure = "Number of unique words", twitter = length(unique(t_words)), blogs = length(unique(b_words)), news = length(unique(n_words))))
```


Look at the distribution of the words
```{r}
library(data.table)
t_wrd_cnt <- table(t_words)
t_td <- data.table(word = names(t_wrd_cnt), count = as.numeric(t_wrd_cnt))
t_td <- t_td[order(-count)]
t_td[,src := "twitter"][, index := seq_len(length(t_td$count))]
df <- data.frame(t_td[1:100])


b_wrd_cnt <- table(b_words)
b_td <- data.table(word = names(b_wrd_cnt), count=as.numeric(b_wrd_cnt))
b_td <- b_td[order(-count)]
b_td[,src := "blogs",][, index := seq_len(length(b_td$count))]
df <- rbind(df, b_td[1:100])

n_wrd_cnt <- table(n_words)
n_td <- data.table(word = names(n_wrd_cnt), count=as.numeric(n_wrd_cnt))
n_td <- n_td[order(-count)]
n_td[,src := "news"][, index := seq_len(length(n_td$count))]
df <- rbind(df, n_td[1:100])

# word distribution of 1000 most common words
#td[, index := seq_len(dim(td)[1])]
ggplot(df, aes(x=index, y=count)) + geom_line() + facet_grid(. ~ src)

```

Add a cummulative frequiency so can see how many words cover 50% and 90% of word occurrences.
```{r}
td[, cum_count := cumsum(td$count)]
tail(td$cum_count,1) %>%
    function (x)
        cat("Number of words for\n\t50% coverage:", min(which(td$cum_count >= x * .5,)),
            "\n\t90% coverage:", min(which(td$cum_count >= x * .9)))
```
