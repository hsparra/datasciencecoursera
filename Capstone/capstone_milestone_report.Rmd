---
title: "Capstone Milestone Report"
output: html_document
---
This report explores the initial exploratory analysis of the English text corpus for the Data Science Capstone course.

# Explore the Data
For this project we are given three files from different sources. See the *Supplemental Information* section for the R code to load the data and create the small er sample files we use for this analysis.
```{r, echo=FALSE}
library(tau)
suppressMessages(library(ggplot2))
library(magrittr)
suppressMessages(library(tm))
```
First we load in the data and get some line counts.
```{r, fig.height=3, cache=TRUE}
# load the data
t_samp <- readLines("twit_samp.txt")
b_samp <- readLines("blog_samp.txt")
n_samp <- readLines("news_samp.txt")

lines <- data.frame(lines = c(length(t_samp), length(b_samp), length(n_samp)))
lines$srce <- c("Twitter", "Blogs", "News")
ggplot(lines, aes(x=as.factor(srce), y=lines)) + geom_bar(stat="identity", fill="darkblue") + labs(x = "Source", y = "Number of Lines", title = "Lines of text by source")
```

Clean up the data a little, such as getting rid of punctuation and non-letter charactrs, then get word counts.
```{r, echo=FALSE}
toSpace <- function(x, pattern) gsub(pattern, " ", x)
cleanText <- function(data) {
    data <- unlist(strsplit(data, split=" "))   %>% 
        tolower                                 %>% 
        remove_stopwords(stopwords())           %>%
        function (x) gsub("[^a-z]", "", x)      %>%
        function (x) gsub("[ ]", "", x)         %>%
        function (x) x[ x != ""]                %>%
        wordStem
}
```
```{r, cache=TRUE}
t_words <- cleanText(t_samp)
b_words <- cleanText(b_samp)
n_words <- cleanText(n_samp)

rbind(data.frame(measure = "Number of words",twitter = length(t_words), blogs = length(b_words), news = length(n_words)),
      data.frame(measure = "Number of unique words", twitter = length(unique(t_words)), blogs = length(unique(b_words)), news = length(unique(n_words))))
```


Look at the distribution of the words
```{r}
library(data.table)
t_wrd_cnt <- table(t_words)
t_td <- data.table(word = names(t_wrd_cnt), count = as.numeric(t_wrd_cnt))
t_td <- t_td[order(-count)]
t_td <- t_td[,src := "twitter"][, index := seq_len(length(t_td$count))]
df <- data.frame(t_td[1:100])


b_wrd_cnt <- table(b_words)
b_td <- data.table(word = names(b_wrd_cnt), count=as.numeric(b_wrd_cnt))
b_td <- b_td[order(-count)]
b_td <- b_td[,src := "blogs",][, index := seq_len(length(b_td$count))]
df <- rbind(df, b_td[1:100])

n_wrd_cnt <- table(n_words)
n_td <- data.table(word = names(n_wrd_cnt), count=as.numeric(n_wrd_cnt))
n_td <- n_td[order(-count)]
n_td <- n_td[,src := "news"][, index := seq_len(length(n_td$count))]
df <- rbind(df, n_td[1:100])


ggplot(df, aes(x=index, y=count)) + geom_line() + facet_grid(. ~ src)

```

Add a cummulative frequiency so can see how many words cover 50% and 90% of word occurrences.
```{r}
showToCover <- function (td) {
    td <- td[, cum_count := cumsum(td$count)]
    tail(td$cum_count,1) %>%
        function (x)
            cat("Number of words for\n\t50% coverage:", min(which(td$cum_count >= x * .5,)),
                "\n\t90% coverage:", min(which(td$cum_count >= x * .9)), 
                "\t which equals", min(which(td$cum_count >= x * .9))/length(td$word), "of unique words"
                )
}

showToCover(t_td)
showToCover(b_td)
showToCover(n_td)

```

```{r}
#crp <- VCorpus(VectorSource(t_clean), readerControl = list(language="english"))
```