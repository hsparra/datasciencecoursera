---
title: "Capstone Milestone Report"
output: html_document
---
This report explores the initial exploratory analysis of the English text corpus for the Data Science Capstone course.

# Explore the Data
For this project we are given three files from different sources. See the *Supplemental Information* section for the R code to load the data and create the small er sample files we use for this analysis.
```{r, echo=FALSE}
library(tau)
suppressMessages(library(ggplot2))
library(magrittr)
suppressMessages(library(tm))
suppressMessages(library(SnowballC))
suppressMessages(library(dplyr))
suppressMessages(library(data.table))
```
First we load in the data and get some line counts.
```{r, fig.height=3, cache=TRUE}
# load the data
t_samp <- readLines("twit_samp.txt")
b_samp <- readLines("blog_samp.txt")
n_samp <- readLines("news_samp.txt")

lines <- data.frame(lines = c(length(t_samp), length(b_samp), length(n_samp)))
lines$srce <- c("Twitter", "Blogs", "News")
ggplot(lines, aes(x=as.factor(srce), y=lines)) + geom_bar(stat="identity", fill="darkblue") + 
    labs(x = "Source", y = "Number of Lines", title = "Lines of text by source")
```

Clean up the data a little, such as getting rid of punctuation and non-letter charactrs, then get word counts.
```{r, echo=FALSE}

cleanText <- function(data, split=FALSE) {
    data <- unlist(strsplit(data, split=" "))   %>% 
        tolower                                 %>% 
        remove_stopwords(stopwords())           %>%
        function (x) gsub("[[:punct:]]", " ", x) %>%
        function (x) gsub("[^a-z]", " ", x)      %>%
        function (x) gsub("[ ]{2}", " ", x)         %>%
        function (x) x[ x != " "]                %>%
        wordStem
    if (split) {
        data <- paste(data, collapse = " ")
    } else {
        data <- unlist(strsplit(data, split = " "))
    }
    data
}
```
```{r, cache=TRUE}
t_words <- cleanText(t_samp)
b_words <- cleanText(b_samp)
n_words <- cleanText(n_samp)

rbind(data.frame(measure = "Number of words",twitter = length(t_words), blogs = length(b_words), news = length(n_words)),
      data.frame(measure = "Number of unique words", twitter = length(unique(t_words)), blogs = length(unique(b_words)), news = length(unique(n_words))))
```


Look at the distribution of the words
```{r}
createTableOfCounts <- function(x, id="default") {
    wrds <- table(x)
    df <- data.table(word = names(wrds), count = as.numeric(wrds)) %>%
        mutate(src = id) %>%
        arrange(desc(count)) %>%
        mutate(index = seq_len(length(count)), cum_count = cumsum(count))
}

t_df <- createTableOfCounts(t_words, "twitter")
b_df <- createTableOfCounts(b_words, "blogs")
n_df <- createTableOfCounts(n_words, "news")

df <- rbind(t_df[1:500], b_df[1:500], n_df[1:500])

ggplot(df, aes(x=index, y=count)) + geom_line() + facet_grid(. ~ src)

```

Add a cummulative frequiency so can see how many words cover 50% and 90% of word occurrences.
```{r, echo=FALSE}
coverageNums <- function( df, percent) {
    val <- tail(df$cum_count, 1)
    words_needed <- min(which(df$cum_count >= val * percent))
    src <- df$src[1]
    data.frame(Source = src, 
               Coverage = paste(percent * 100, "%", sep=""),
               Words_Needed= words_needed, 
               Percent_of_Unique_Words = paste(format(words_needed / length(df$word) * 100, digits=0), "%", sep=""))
}
```
```{r}
coverage_df <- coverageNums(t_df, .5) %>% rbind(coverageNums(b_df, .5)) %>% rbind(coverageNums(n_df, .5)) %>% 
    rbind(coverageNums(t_df, .9)) %>% rbind(coverageNums(b_df, .9)) %>%rbind(coverageNums(n_df, .9))

coverage_df

```

```{r, cache=TRUE, echo=FALSE}
# own n_gram creator given a vector sentences
createNGram <- function(inSentence, ngramNum = 2) {
    x <- unlist(strsplit(inSentence, " "))
    y <- paste(x[seq(from=1, to=length(x) - 1, by=1)], x[seq(from=2, to=length(x), by=1)], sep = " ")
}

#x <- c("A first line to parse", "A second line to parse", "Yet another something to work #2 with!!")
#y <- lapply(x, createNGram)
#y <- sapply(x, createNGram)   # will create vector of NGrams if x is a vectoy
z <- sapply(t_samp, cleanText, TRUE)
#z <- sapply(t_samp, cleanText) %>% sapply(createNGram) %>% unlist  # create vector of NGrams

#crp <- VCorpus(VectorSource(t_clean), readerControl = list(language="english"))
suppressMessages(library(RWeka))
bigram_rw <- NGramTokenizer(z, control=Weka_control(min=2, max=2, dilimeters = " "))
t_bi <- createTableOfCounts(bigram_rw, "twitter")
b_bi <- sapply(b_samp, cleanText, TRUE) %>%
    NGramTokenizer(control=Weka_control(min=2, max=2, dilimeters = " ")) %>%
    createTableOfCounts("blogs")
n_bi <- sapply(n_samp, cleanText, TRUE) %>%
    NGramTokenizer(control=Weka_control(min=2, max=2, dilimeters = " ")) %>%
    createTableOfCounts("news")

# Graph distro
bi_df <- rbind(t_bi[1:500], b_bi[1:500], n_bi[1:500])

ggplot(bi_df, aes(x=index, y=count)) + geom_line() + facet_grid(. ~ src)

coverage_bi <- coverageNums(t_bi, .5) %>% rbind(coverageNums(b_bi, .5)) %>% rbind(coverageNums(n_bi, .5)) %>% 
    rbind(coverageNums(t_bi, .9)) %>% rbind(coverageNums(b_bi, .9)) %>%rbind(coverageNums(n_bi, .9))

coverage_bi
# from tau library
#bigrams <- textcnt(z, method="string", n=2L)
```