---
title: "Weight Lifting Exercise Quality Prediction"
output: html_document
---
## Synopsis
In recent years human activity research as moved from predicting what activity an individual is performing to trying to predict how well an individual is performing an activity. In [Qualitative Activity Recognition of Weight Lifting Exercises](http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201) Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013, Velloso, Bulling, Gellersen, Ugulino, and Fuks explored predicting how well participants performed a specific set of dumbbell exercises. Individuals were asked to perform barbell lifts correctly and incorrectly in 5 different ways and data was captured while they performed the lists.

This report documents processing data from this research to predict which way an individual was performing an exercise. Data from this was obtained from the [Human Activities Recognition (HAR) project](http://groupware.les.inf.puc-rio.br/har). 

From the training data a variables were identified and a model was built to predict which way an exercise was performed. Usng the test data, the model correctly predicted which method was being used in the exercise in 20 out of 20 of the test cases. This indicates that it is possible with the appropriate instrumentation to identify if an individual is performing a specific set of dumbbell exercises correctly, and if not, identify the most common mistakes.

#### Quick Model Summary
The final model was created using the `caret` package and a `random forest` model. Default values were used which resulted in the `bootstrap resampling` being used for the `cross validation`. The expected out of sample error of the model is `0.6%` and the expected accuracy is `.993`. 

## Loading and Processing the Data
Training data was obtained at [https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv), and the testing data was obtained from [https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv).

### Reading the data
First we read in the downloaded data. Prior to processing the downloaded data files were saved in a .zip archive to conserve space.
```{r, cache=TRUE}
library(caret)
## Comment out if not able to do multicore processing
library(doMC)
registerDoMC(2)

data <- read.csv(unz("data/pml-data.zip","pml-training.csv"),stringsAsFactors=F)
test.final <- read.csv(unz("data/pml-data.zip","pml-testing.csv"),stringsAsFactors=F)
```

Once loaded we check the dimensions of the data.
```{r}
dim(data)
dim(test.final)
```

As can be seen, both the training data and the test data have 160 variables. The variable specifying the method used in the dumbell is the `classe` variable, which is the 160th variable.
```{r}
data[1, 159:160]
```

Further observation of the variables showed that the first seven variables were associated with user and time and so they were removed since the goal was to predict based on the sensor readings.
```{r}
data[1:2, 1:7]
data <- data[, -c(1:7)]
```

### Split Training Dataset
Next, the full training dataset was split into a training dataset and a test data set to use for validating the model. Seventy percent of the data was used for the training data. This would give more accuracy than a 60/40 split but with higher variability.
```{r}
library(caret)
inTrain <- createDataPartition(y=data$classe, p=0.7, list=FALSE)
train <- data[inTrain,]
test <- data[-inTrain,]
```


### Select Variables for Model
Next, near zero covariates were identified and removed.
```{r, cache=TRUE}
nsv <- nearZeroVar(train)
train <- train[,-nsv]
```

Further observation of data showed that some variables had excessive number of NAs (> 95%). These were identified and removed. The function used to identify the NAs can be found in the Appendix.
```{r, echo=FALSE, cache=TRUE}
naCheck <- function (df, check) {
  y <- character()
  z <- numeric()
  x <- numeric()
  for (i in 1:length(names(df))) {
    w <- sum(is.na(df[,i]))/length(df[,i])
    if (w > check) {
      x <- append(x, i)
      y <- append(y,names(df)[i])
      z <- append(z,w)
    }
  }
  data.frame(colNum=x, colName=y, percent=z, stringsAsFactors=FALSE)
}
```
```{r}
lotsNas <- naCheck(train, .95)
train <- train[,-lotsNas$colNum]
```

Next, integer columns were removed. They could have been converted but intial analysis with a subset of the training data indicated that they did not material add to the model. The function to identify the variables can be found in the Appendix.
```{r, echo=FALSE, cache=TRUE}
colsOfType <- function (df, type) {
  x <- integer()
  for (i in 1:length(df)) {
    if (class(df[,i]) == type)
      x <- append(x, i)
  }
  x
}
```

```{r}
colsToRemove <- colsOfType(train, "integer")
train <- train[,-colsToRemove]
```

Finally, variables that were highly correlated with each other were removed. The `classe` variable was excluded in the correlation search since that is the variable we are interested in and variables highly correlated with the `classe` variable could indicate possible good predictors.
```{r}
m <- abs(cor(train[,-28]))
diag(m) <- 0
rowsLittleCor <- row.names(m)[m[,dim(m)[2]] > .8]
train <- train[,!(colnames(train) %in% rowsLittleCor[-length(rowsLittleCor)])]
```

As a final step we converted the `classe` variable to a factor.
```{r}
train$classe <- as.factor(train$classe)
```


## Creating the Model
Intial model exploration was done using a random 2000 observations from the training dataset (70%) derived when splitting the full training dataset. This small subset was used due to the excessive processing required to use the full dataset on the available hardware. Using only 2000 observations still took between fifteen and twenty minutes per run. By reducing processing time we were able to more quickly iterate through different models and experiment before moving to tuning the final model. This exploration lead to the selection of using the random forest algorigthm to create the prediction model. Once this selection was made the 70% training dataset was processed as above and fed as input into the randeom forest algorithm.
```{r, eval=FALSE}
modFit <- train(classe ~ ., data=train, method="rf", prox=TRUE)
```

### Model Analysis
The defaults in the `caret` package were used with meant bootstrap resampling with 25 repetitions was used and 500 trees. In the `caret` package when using the `train()` function with a `random forest` model, cross validation occurs as part of the random forest algorithm. The resulting model is below.
```{r, eval=FALSE}
modFit
```
```
Random Forest 

13737 samples
   27 predictors
    5 classes: 'A', 'B', 'C', 'D', 'E' 

No pre-processing
Resampling: Bootstrapped (25 reps) 

Summary of sample sizes: 13737, 13737, 13737, 13737, 13737, 13737, ... 

Resampling results across tuning parameters:

  mtry  Accuracy  Kappa  Accuracy SD  Kappa SD
  2     0.99      0.988  0.0015       0.0019  
  14    0.989     0.986  0.00178      0.00225 
  27    0.984     0.98   0.00285      0.0036  

Accuracy was used to select the optimal model using  the largest value.
The final value used for the model was mtry = 2. 
```
### Expected Out of Sample Error
As can be seen from the final model, the expected out of sample error is `0.6%`.
```{r, eval=FALSE}
modFit$finalModel
```
```
Call:
 randomForest(x = x, y = y, mtry = param$mtry, proximity = TRUE) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 2

        OOB estimate of  error rate: 0.6%
Confusion matrix:
     A    B    C    D    E  class.error
A 3903    1    0    0    2 0.0007680492
B   15 2630   13    0    0 0.0105342363
C    0   10 2373   13    0 0.0095993322
D    0    0   20 2230    2 0.0097690941
E    0    0    1    5 2519 0.0023762376
```


## Model Testing
The reserved test dataaset from the segmentation of the original full training dataset was then used to test the resulting model. The columns removed from the training dataset were also removed from the test dataset.
```{r, echo=FALSE}
test <- test[,-nsv]
test <- test[,-lotsNas$colNum]
test <- test[,-colsToRemove]
test$classe <- as.factor(test$classe)
```

Then the test data was used to predict using the model and the confusion matrix was observed.
```{r, eval=FALSE}
pred <- predict(modFit, test);
```
```{r, eval=FALSE}
confusionMatrix(pred, test$classe)
```

```
Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 1672    7    0    0    0
         B    1 1129    3    0    0
         C    0    3 1015   13    1
         D    0    0    8  951    4
         E    1    0    0    0 1077

Overall Statistics
                                         
               Accuracy : 0.993          
                 95% CI : (0.9906, 0.995)
    No Information Rate : 0.2845         
    P-Value [Acc > NIR] : < 2.2e-16      
                                         
                  Kappa : 0.9912         
 Mcnemar's Test P-Value : NA             

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.9988   0.9912   0.9893   0.9865   0.9954
Specificity            0.9983   0.9992   0.9965   0.9976   0.9998
Pos Pred Value         0.9958   0.9965   0.9835   0.9875   0.9991
Neg Pred Value         0.9995   0.9979   0.9977   0.9974   0.9990
Prevalence             0.2845   0.1935   0.1743   0.1638   0.1839
Detection Rate         0.2841   0.1918   0.1725   0.1616   0.1830
Detection Prevalence   0.2853   0.1925   0.1754   0.1636   0.1832
Balanced Accuracy      0.9986   0.9952   0.9929   0.9920   0.9976
```

An accuracy of `0.993` and a kappa of `0.9912` were achieved.

### Most important variables
The 20 most important variables in the model are:
```{r, eval=FALSE}
varImp(modFit)
```
```
rf variable importance

  only 20 most important variables shown (out of 27)

                  Overall
roll_belt          100.00
yaw_belt            78.91
magnet_dumbbell_z   61.87
pitch_forearm       61.70
pitch_belt          55.43
roll_dumbbell       44.11
roll_forearm        42.63
roll_arm            31.21
gyros_belt_z        29.18
yaw_dumbbell        27.41
magnet_forearm_z    27.14
gyros_dumbbell_y    26.93
yaw_arm             26.12
pitch_dumbbell      24.99
magnet_forearm_y    23.17
yaw_forearm         18.98
pitch_arm           13.63
gyros_arm_y         11.72
gyros_dumbbell_x    11.17
gyros_arm_x         10.92
```

## Conclusion
It appears that it is possible to construct a predictive model that utilizes sensor input to predict the quality of the exercise performed on a given set of dumbbell exercises. This indicates that this approach might be feasible for a wider range of exercises.



# Apppendix
### Function to identify excessive NAs.
```
naCheck <- function (df, check) {
  y <- character()
  z <- numeric()
  x <- numeric()
  for (i in 1:length(names(df))) {
    w <- sum(is.na(df[,i]))/length(df[,i])
    if (w > check) {
      x <- append(x, i)
      y <- append(y,names(df)[i])
      z <- append(z,w)
    }
  }
  data.frame(colNum=x, colName=y, percent=z, stringsAsFactors=FALSE)
}
```
### Function to remove variables
```
colsOfType <- function (df, type) {
  x <- integer()
  for (i in 1:length(df)) {
    if (class(df[,i]) == type)
      x <- append(x, i)
  }
  x
}
```